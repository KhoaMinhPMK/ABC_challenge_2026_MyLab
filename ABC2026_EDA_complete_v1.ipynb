{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL A.1: SETUP & CONFIGURATION + PLOT UTILITIES\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "\n",
    "# --- AUTO-DETECT ENVIRONMENT ---\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Colab: Download data if needed\n",
    "    import requests, zipfile\n",
    "    DATA_URL = \"https://bkuteam.site/data.zip\"\n",
    "    BASE_DIR = Path(\"/content/ABC2026\")\n",
    "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_ROOT = BASE_DIR / \"ABC2026 Sozolab Challenge\"\n",
    "    DATASET_DIR = DATA_ROOT / \"Dataset\"\n",
    "    \n",
    "    if not (DATASET_DIR.exists() and any(DATASET_DIR.glob(\"*.csv\"))):\n",
    "        print(\"ğŸ“¥ Downloading data.zip...\")\n",
    "        ZIP_PATH = BASE_DIR / \"data.zip\"\n",
    "        resp = requests.get(DATA_URL, stream=True)\n",
    "        resp.raise_for_status()\n",
    "        with open(ZIP_PATH, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk: f.write(chunk)\n",
    "        print(\"ğŸ“¦ Extracting...\")\n",
    "        with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "            zf.extractall(BASE_DIR)\n",
    "    print(f\"âœ… [COLAB] Data ready at: {DATASET_DIR}\")\n",
    "else:\n",
    "    # Local: Use workspace path\n",
    "    WORKSPACE = Path(r\"E:\\project\\ABC2026\")\n",
    "    DATA_ROOT = WORKSPACE / \"ABC2026 Sozolab Challenge\"\n",
    "    DATASET_DIR = DATA_ROOT / \"Dataset\"\n",
    "    print(f\"âœ… [LOCAL] Data at: {DATASET_DIR}\")\n",
    "\n",
    "DATA_PATH = str(DATASET_DIR)\n",
    "OUTPUT_PATH = str(DATA_ROOT)\n",
    "\n",
    "# --- CREATE FIGURES DIRECTORY ---\n",
    "FIGURES_DIR = DATA_ROOT / \"figures\"\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "print(f\"ğŸ“Š Figures will be saved to: {FIGURES_DIR}\")\n",
    "\n",
    "# --- PLOT METADATA TRACKER ---\n",
    "PLOT_METADATA = []\n",
    "\n",
    "def log_plot(plot_name, filename, data_info, insights):\n",
    "    \"\"\"\n",
    "    Utility to log plot metadata.\n",
    "    \n",
    "    Parameters:\n",
    "    - plot_name: TÃªn biá»ƒu Ä‘á»“ (human-readable)\n",
    "    - filename: TÃªn file Ä‘Ã£ lÆ°u\n",
    "    - data_info: Dict chá»©a thÃ´ng tin vá» data (shape, columns, etc.)\n",
    "    - insights: Insight chÃ­nh tá»« biá»ƒu Ä‘á»“\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"plot_name\": plot_name,\n",
    "        \"filename\": filename,\n",
    "        \"data_info\": data_info,\n",
    "        \"insights\": insights\n",
    "    }\n",
    "    PLOT_METADATA.append(metadata)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ“Š Biá»ƒu Ä‘á»“: {plot_name}\")\n",
    "    print(f\"ğŸ“ File: {filename}\")\n",
    "    print(f\"ğŸ“ˆ Dá»¯ liá»‡u: {data_info}\")\n",
    "    print(f\"ğŸ” Insight: {insights}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def save_plot_metadata():\n",
    "    \"\"\"LÆ°u táº¥t cáº£ metadata plots vÃ o JSON.\"\"\"\n",
    "    metadata_path = DATA_ROOT / \"eda_plots_metadata_v1.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(PLOT_METADATA, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ğŸ’¾ Plot metadata saved: {metadata_path}\")\n",
    "\n",
    "# --- BEACON MAPPING (25 Beacons) ---\n",
    "MAC_LIST = [\n",
    "    'F7:7F:78:76:7E:F3', 'C6:CD:5E:3D:2F:BB', 'D6:F4:3A:79:74:63', 'C9:17:55:E2:3E:0E', 'CA:60:AB:EE:EC:7F',\n",
    "    'D6:51:7F:AB:0E:29', 'CC:54:33:F6:A7:90', 'EB:20:56:87:04:5A', 'EE:E7:46:DC:19:6F', 'C8:5B:BF:37:07:A0',\n",
    "    'D7:26:F6:A3:44:D2', 'DD:83:B0:27:FD:36', 'E5:CD:4A:36:87:06', 'DC:22:B8:17:4E:B5', 'EA:09:20:80:D6:44',\n",
    "    'E6:99:D1:EC:C6:81', 'F6:DA:97:C7:D5:28', 'EA:66:A1:12:2C:F4', 'C9:EA:57:8B:0F:80', 'D6:7C:1D:2C:2A:0A',\n",
    "    'DA:E1:70:5F:44:97', 'DD:10:10:F6:4F:27', 'E6:F3:93:A8:9E:22', 'E6:60:05:1F:88:F9', 'D4:33:FD:F4:C2:A8'\n",
    "]\n",
    "BEACON_IDS = [str(i) for i in range(1, 26)]\n",
    "MAC_TO_BEACON = dict(zip(MAC_LIST, BEACON_IDS))\n",
    "\n",
    "# --- FINAL OUTPUT ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸŒ Environment: {'COLAB' if IS_COLAB else 'LOCAL'}\")\n",
    "print(f\"ğŸ“‚ Data Path: {DATA_PATH}\")\n",
    "print(f\"ğŸ“‚ Output Path: {OUTPUT_PATH}\")\n",
    "print(f\"ğŸ“¡ Beacons: {len(BEACON_IDS)} beacons mapped\")\n",
    "print(f\"ğŸ“Š Figures Directory: {FIGURES_DIR}\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… Setup Complete! Ready for EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL A.2: LOAD FULL DATA (PAIRWISE CONCATENATION)\n",
    "# =============================================================================\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ”„ Loading FULL Raw Data (Pairwise Concatenation)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- LOAD LABELS ---\n",
    "labels_path = Path(DATA_PATH) / \"5f_label_loc_train.csv\"\n",
    "df_label = pd.read_csv(labels_path)\n",
    "print(f\"âœ… Loaded Labels: {labels_path.name}\")\n",
    "print(f\"   Shape: {df_label.shape}\")\n",
    "\n",
    "# --- LOAD BLE FILES (BATCHED) ---\n",
    "ble_dir = Path(DATA_PATH) / \"BLE Data\"\n",
    "ble_files = list(ble_dir.glob(\"*.csv\"))\n",
    "print(f\"\\nğŸ“¡ Total BLE files: {len(ble_files)}\")\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "print(f\"\\nğŸ”„ Loading in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "all_batches = []\n",
    "total_records = 0\n",
    "\n",
    "for batch_num in range(0, len(ble_files), BATCH_SIZE):\n",
    "    batch_files = ble_files[batch_num:batch_num + BATCH_SIZE]\n",
    "    batch_chunks = []\n",
    "    \n",
    "    for file_path in batch_files:\n",
    "        try:\n",
    "            df_chunk = pd.read_csv(file_path)\n",
    "            batch_chunks.append(df_chunk)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if batch_chunks:\n",
    "        batch_df = pd.concat(batch_chunks, ignore_index=True)\n",
    "        all_batches.append(batch_df)\n",
    "        total_records += len(batch_df)\n",
    "        \n",
    "        print(f\"   Batch {batch_num//BATCH_SIZE + 1}/{(len(ble_files)-1)//BATCH_SIZE + 1}: \"\n",
    "              f\"{len(batch_chunks)} files â†’ {len(batch_df):,} records (Total: {total_records:,})\")\n",
    "        \n",
    "        del batch_chunks, batch_df\n",
    "        gc.collect()\n",
    "\n",
    "# --- PAIRWISE CONCATENATION (KEY FIX!) ---\n",
    "print(f\"\\nğŸ”— Pairwise concatenation ({len(all_batches)} batches)...\")\n",
    "\n",
    "while len(all_batches) > 1:\n",
    "    new_batches = []\n",
    "    \n",
    "    for i in range(0, len(all_batches), 2):\n",
    "        if i + 1 < len(all_batches):\n",
    "            # Merge pair\n",
    "            merged = pd.concat([all_batches[i], all_batches[i+1]], \n",
    "                             ignore_index=True, copy=False)\n",
    "            new_batches.append(merged)\n",
    "            del all_batches[i], all_batches[i+1]\n",
    "        else:\n",
    "            # Odd one out\n",
    "            new_batches.append(all_batches[i])\n",
    "    \n",
    "    all_batches = new_batches\n",
    "    gc.collect()\n",
    "    print(f\"   â†’ Merged to {len(all_batches)} chunk(s)\")\n",
    "\n",
    "df_ble = all_batches[0]\n",
    "del all_batches\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… All BLE data loaded!\")\n",
    "print(f\"   Total records: {len(df_ble):,}\")\n",
    "print(f\"   Shape: {df_ble.shape}\")\n",
    "print(f\"   Memory: {df_ble.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# --- CONVERT MAC TO BEACON ID ---\n",
    "print(\"\\nğŸ”§ Converting MAC addresses...\")\n",
    "df_ble['beacon_id'] = df_ble['mac'].map(MAC_TO_BEACON)\n",
    "\n",
    "unmapped = df_ble['beacon_id'].isna().sum()\n",
    "if unmapped > 0:\n",
    "    print(f\"âš ï¸  Dropping {unmapped:,} unmapped ({100*unmapped/len(df_ble):.2f}%)\")\n",
    "    df_ble = df_ble.dropna(subset=['beacon_id'])\n",
    "else:\n",
    "    print(\"âœ… All MACs mapped\")\n",
    "\n",
    "print(f\"   Final shape: {df_ble.shape}\")\n",
    "\n",
    "# --- STATS ---\n",
    "print(\"\\nğŸ“Š BLE Summary:\")\n",
    "print(f\"   Unique beacons: {df_ble['beacon_id'].nunique()}\")\n",
    "print(f\"   RSSI: [{df_ble['rssi'].min():.1f}, {df_ble['rssi'].max():.1f}] dBm (mean={df_ble['rssi'].mean():.2f})\")\n",
    "\n",
    "print(\"\\nğŸ“Š Label Summary:\")\n",
    "print(f\"   Unique locations: {df_label['room'].nunique() if 'room' in df_label.columns else df_label.columns}\")\n",
    "print(f\"   Total labels: {len(df_label):,}\")\n",
    "\n",
    "# --- PREVIEW ---\n",
    "print(\"\\nğŸ“‹ BLE Preview:\")\n",
    "print(df_ble.head(3))\n",
    "\n",
    "print(\"\\nğŸ“‹ Label Preview:\")\n",
    "print(df_label.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… FULL data loaded!\")\n",
    "print(f\"ğŸ’¾ df_ble: {df_ble.shape}\")\n",
    "print(f\"ğŸ’¾ df_label: {df_label.shape}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL A.2: LOAD RAW DATA (MEMORY-EFFICIENT VERSION)\n",
    "# =============================================================================\n",
    "import gc  # Garbage collection\n",
    "\n",
    "print(\"ğŸ”„ Loading Raw Data (Memory-Efficient)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- LOAD LABELS ---\n",
    "labels_path = Path(DATA_PATH) / \"5f_label_loc_train.csv\"\n",
    "df_label = pd.read_csv(labels_path)\n",
    "print(f\"âœ… Loaded Labels: {labels_path.name}\")\n",
    "print(f\"   Shape: {df_label.shape}\")\n",
    "print(f\"   Columns: {list(df_label.columns)}\")\n",
    "\n",
    "# --- LOAD BLE FILES (BATCHED) ---\n",
    "ble_dir = Path(DATA_PATH) / \"BLE Data\"\n",
    "ble_files = list(ble_dir.glob(\"*.csv\"))\n",
    "print(f\"\\nğŸ“¡ BLE Data directory: {ble_dir.name}/\")\n",
    "print(f\"   Total BLE files: {len(ble_files)}\")\n",
    "\n",
    "# Load in batches to avoid memory overflow\n",
    "BATCH_SIZE = 500\n",
    "print(f\"\\nğŸ”„ Loading BLE files in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "all_ble_data = []\n",
    "total_records = 0\n",
    "\n",
    "for batch_num in range(0, len(ble_files), BATCH_SIZE):\n",
    "    batch_files = ble_files[batch_num:batch_num + BATCH_SIZE]\n",
    "    batch_chunks = []\n",
    "    \n",
    "    for file_path in batch_files:\n",
    "        try:\n",
    "            df_chunk = pd.read_csv(file_path)\n",
    "            batch_chunks.append(df_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Skip {file_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate batch\n",
    "    if batch_chunks:\n",
    "        batch_df = pd.concat(batch_chunks, ignore_index=True)\n",
    "        all_ble_data.append(batch_df)\n",
    "        total_records += len(batch_df)\n",
    "        \n",
    "        print(f\"   Batch {batch_num//BATCH_SIZE + 1}/{(len(ble_files)-1)//BATCH_SIZE + 1}: \"\n",
    "              f\"{len(batch_chunks)} files â†’ {len(batch_df):,} records \"\n",
    "              f\"(Total: {total_records:,})\")\n",
    "        \n",
    "        # Free memory\n",
    "        del batch_chunks, batch_df\n",
    "        gc.collect()\n",
    "\n",
    "# Final concatenation\n",
    "print(\"\\nğŸ”— Concatenating all batches...\")\n",
    "df_ble = pd.concat(all_ble_data, ignore_index=True)\n",
    "del all_ble_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ… All BLE data loaded!\")\n",
    "print(f\"   Total records: {len(df_ble):,}\")\n",
    "print(f\"   Shape: {df_ble.shape}\")\n",
    "print(f\"   Columns: {list(df_ble.columns)}\")\n",
    "print(f\"   Memory usage: {df_ble.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# --- CONVERT MAC TO BEACON ID ---\n",
    "print(\"\\nğŸ”§ Converting MAC addresses to Beacon IDs...\")\n",
    "df_ble['beacon_id'] = df_ble['mac'].map(MAC_TO_BEACON)\n",
    "\n",
    "# Check unmapped\n",
    "unmapped_count = df_ble['beacon_id'].isna().sum()\n",
    "if unmapped_count > 0:\n",
    "    unmapped_pct = 100 * unmapped_count / len(df_ble)\n",
    "    print(f\"âš ï¸  Unmapped MACs: {unmapped_count:,} ({unmapped_pct:.2f}%)\")\n",
    "    df_ble = df_ble.dropna(subset=['beacon_id'])\n",
    "    print(f\"   Shape after dropping: {df_ble.shape}\")\n",
    "else:\n",
    "    print(\"âœ… All MACs mapped successfully\")\n",
    "\n",
    "# --- BASIC STATS ---\n",
    "print(\"\\nğŸ“Š BLE Data Summary:\")\n",
    "print(f\"   Unique beacons: {df_ble['beacon_id'].nunique()}\")\n",
    "print(f\"   RSSI range: [{df_ble['rssi'].min():.1f}, {df_ble['rssi'].max():.1f}]\")\n",
    "print(f\"   Mean RSSI: {df_ble['rssi'].mean():.2f} dBm\")\n",
    "\n",
    "print(\"\\nğŸ“Š Label Data Summary:\")\n",
    "print(f\"   Unique locations: {df_label['location'].nunique()}\")\n",
    "print(f\"   Total labels: {len(df_label):,}\")\n",
    "\n",
    "# --- PREVIEW ---\n",
    "print(\"\\nğŸ“‹ BLE Preview:\")\n",
    "print(df_ble[['timestamp', 'mac', 'rssi', 'beacon_id']].head(3))\n",
    "\n",
    "print(\"\\nğŸ“‹ Label Preview:\")\n",
    "print(df_label.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Data loaded successfully!\")\n",
    "print(f\"ğŸ’¾ df_ble: {df_ble.shape}\")\n",
    "print(f\"ğŸ’¾ df_label: {df_label.shape}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
